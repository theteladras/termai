"""Command generation from natural language instructions.

Uses a local LLM to convert user intent into shell commands.  Falls back
to a simple keyword-based mapper when the model is unavailable so the
tool stays functional even without AI.

When a remote AI provider (OpenAI/Claude) is configured, the local model
generates first, then a complexity classifier decides whether to delegate
to the remote provider for higher-quality results.
"""

from __future__ import annotations

import re
from typing import TYPE_CHECKING

from termai.model import LocalModel

if TYPE_CHECKING:
    from termai.context import SessionContext

CYAN = "\033[1;36m"
DIM = "\033[2m"
YELLOW = "\033[0;33m"
RESET = "\033[0m"

_model: LocalModel | None = None
_force_mode: str | None = None  # "remote", "local", or None (auto)


def set_force_mode(mode: str | None) -> None:
    """Override delegation: 'remote', 'local', or None for auto."""
    global _force_mode
    _force_mode = mode


def _get_model() -> LocalModel:
    global _model
    if _model is None:
        _model = LocalModel()
    return _model


def generate_command(instruction: str, ctx: "SessionContext") -> str | None:
    """Convert a natural language instruction into a shell command.

    Flow:
    1. Try local generation (LLM or keyword fallback)
    2. If remote AI is configured, classify complexity
    3. Delegate to remote if complex; fall back to local on remote failure
    """
    model = _get_model()
    from_fallback = False

    if _force_mode == "remote":
        return _generate_with_remote(instruction, ctx, local_result=None)

    if model.is_available and _force_mode != "local":
        local_result = _generate_with_llm(instruction, ctx, model)
    elif model.is_available:
        return _generate_with_llm(instruction, ctx, model)
    else:
        local_result = _generate_fallback(instruction, ctx)
        from_fallback = True

    if _force_mode == "local":
        return local_result

    from termai.remote import get_remote_provider
    remote = get_remote_provider()
    if remote and remote.is_available():
        from termai.classifier import classify
        decision = classify(instruction, local_result, from_fallback=from_fallback)
        if decision == "remote":
            remote_result = _generate_with_remote(instruction, ctx, local_result=local_result)
            if remote_result is not None:
                return remote_result

    return local_result


def _generate_with_llm(
    instruction: str,
    ctx: "SessionContext",
    model: LocalModel,
) -> str | None:
    """Use the local LLM to produce a shell command."""
    system_prompt = ctx.as_system_prompt()
    user_prompt = f"Instruction: {instruction}"

    raw = model.generate(system_prompt, user_prompt, max_tokens=256)
    if not raw:
        print("[termai] Model returned empty response — falling back.")
        return _generate_fallback(instruction, ctx)

    command = _clean_model_output(raw)
    return command or _generate_fallback(instruction, ctx)


def _generate_with_remote(
    instruction: str,
    ctx: "SessionContext",
    *,
    local_result: str | None,
) -> str | None:
    """Delegate command generation to the remote AI provider."""
    from termai.remote import get_remote_provider
    remote = get_remote_provider()
    if not remote:
        return None

    print(f"  {CYAN}[remote]{RESET} {DIM}Processing with remote AI...{RESET}")

    system_prompt = ctx.as_system_prompt()
    user_prompt = f"Instruction: {instruction}"

    try:
        raw = remote.generate(system_prompt, user_prompt, max_tokens=512)
        if not raw:
            print(f"  {YELLOW}[remote]{RESET} {DIM}Empty response — using local result{RESET}")
            return local_result
        command = _clean_model_output(raw)
        if command:
            print(f"  {CYAN}[remote]{RESET} {DIM}Command generated by remote AI{RESET}")
            return command
        return local_result
    except Exception as e:
        print(f"  {YELLOW}[remote]{RESET} {DIM}Remote AI failed: {e} — using local result{RESET}")
        return local_result


def _clean_model_output(raw: str) -> str:
    """Strip markdown fences, leading $, and excess whitespace."""
    text = raw.strip()

    fence_pattern = re.compile(r"^```(?:bash|sh|zsh)?\s*\n?(.*?)\n?```$", re.DOTALL)
    m = fence_pattern.match(text)
    if m:
        text = m.group(1).strip()

    if text.startswith("$ "):
        text = text[2:]

    lines = [ln for ln in text.splitlines() if not ln.startswith("#")]
    return "\n".join(lines).strip()


# ---------------------------------------------------------------------------
# Rule-based fallback (no AI required)
# ---------------------------------------------------------------------------

_FALLBACK_MAP: list[tuple[list[str], str]] = [
    (["list", "files"],            "ls -la"),
    (["list", "directory"],        "ls -la"),
    (["disk", "usage"],            "df -h"),
    (["disk", "space"],            "du -sh *"),
    (["memory", "usage"],          "free -h" if __import__("platform").system() == "Linux" else "vm_stat"),
    (["current", "directory"],     "pwd"),
    (["network", "interfaces"],    "ifconfig" if __import__("platform").system() == "Darwin" else "ip addr"),
    (["running", "processes"],     "ps aux"),
    (["system", "info"],           "uname -a"),
    (["find", "python", "files"],  'find . -name "*.py" -type f'),
    (["find", "log", "files"],     'find . -name "*.log" -type f'),
    (["count", "lines"],           "wc -l"),
    (["git", "status"],            "git status"),
    (["git", "log"],               "git log --oneline -10"),
    (["git", "history"],           "git log --oneline -10"),
    (["docker", "containers"],     "docker ps -a"),
    (["docker", "images"],         "docker images"),
    (["command", "history"],       "cat ~/.zsh_history | tail -30" if __import__("os").environ.get("SHELL", "").endswith("zsh") else "cat ~/.bash_history | tail -30"),
    (["history"],                  "cat ~/.zsh_history | tail -30" if __import__("os").environ.get("SHELL", "").endswith("zsh") else "cat ~/.bash_history | tail -30"),
    (["whoami"],                   "whoami"),
    (["uptime"],                   "uptime"),
    (["date"],                     "date"),
    (["hostname"],                 "hostname"),
    (["cpu", "info"],              "sysctl -n machdep.cpu.brand_string" if __import__("platform").system() == "Darwin" else "lscpu"),
    (["open", "ports"],            "lsof -i -P -n | grep LISTEN"),
    (["environment", "variables"], "env"),
    (["path"],                     "echo $PATH | tr ':' '\\n'"),
]


def _generate_fallback(instruction: str, ctx: "SessionContext") -> str:
    """Keyword-matching fallback when no AI model is loaded."""
    words = instruction.lower().split()
    best_match = ""
    best_score = 0.0

    for keywords, cmd in _FALLBACK_MAP:
        hits = sum(1 for kw in keywords if kw in words)
        if hits == 0:
            continue
        # Prefer rules where ALL keywords match (ratio == 1.0),
        # breaking ties by absolute number of matched keywords.
        ratio = hits / len(keywords)
        score = ratio + hits * 0.01
        if score > best_score:
            best_score = score
            best_match = cmd

    if best_match:
        print("[termai] (using rule-based fallback)")
        return best_match

    print("[termai] Could not generate a command. Try rephrasing or install a local model.")
    return None  # type: ignore[return-value]
