"""Interactive chat mode.

Launched with `termai --chat`.  Provides a REPL where the user can ask
questions, request commands, and execute them after confirmation.
Supports multi-turn conversation and slash commands for control.
"""

from __future__ import annotations

import re
from typing import TYPE_CHECKING

from termai.model import LocalModel
from termai.executor import preview_and_execute, preview_and_execute_batch
from termai.plugins import get_registry

if TYPE_CHECKING:
    from termai.context import SessionContext

BOLD = "\033[1m"
CYAN = "\033[1;36m"
GREEN = "\033[1;32m"
YELLOW = "\033[0;33m"
MAGENTA = "\033[1;35m"
DIM = "\033[2m"
RESET = "\033[0m"

CHAT_SYSTEM_PROMPT = (
    "You are termai, a helpful AI terminal assistant running inside the user's shell. "
    "You can answer general questions, explain concepts, and suggest shell commands.\n"
    "\n"
    "When the user asks you to DO something on their system, respond with ONLY the "
    "shell command(s) wrapped in a ```bash code fence. Example:\n"
    "```bash\nls -la /tmp\n```\n"
    "\n"
    "When the user asks a QUESTION or wants an explanation, respond in plain text.\n"
    "Be concise. Prefer one-liners when possible.\n"
)

HELP_TEXT = f"""
{CYAN}termai interactive chat{RESET}
{DIM}──────────────────────────────────{RESET}
  Type natural language to ask questions or request commands.
  Commands generated by the AI are shown for preview before execution.

  {BOLD}Slash commands:{RESET}
    /help      Show this help message
    /history   Show session command history
    /context   Show current system context
    /clear     Clear conversation history
    exit       Quit interactive mode
"""


def interactive_chat(ctx: "SessionContext") -> None:
    """Run the interactive chat REPL."""
    print(f"\n{MAGENTA}{'═' * 50}{RESET}")
    print(f"{MAGENTA}  termai — interactive chat mode{RESET}")
    print(f"{MAGENTA}{'═' * 50}{RESET}")
    print(f"  Type {BOLD}/help{RESET} for commands, {BOLD}exit{RESET} to quit.\n")

    model = LocalModel()
    messages: list[dict[str, str]] = []

    try:
        while True:
            try:
                user_input = input(f"{GREEN}ai>{RESET} ").strip()
            except EOFError:
                break

            if not user_input:
                continue

            if user_input.lower() in ("exit", "quit"):
                break

            if user_input.startswith("/"):
                _handle_slash_command(user_input, ctx, messages)
                continue

            response = _get_response(user_input, ctx, model, messages)

            messages.append({"role": "user", "content": user_input})
            messages.append({"role": "assistant", "content": response})

            commands = _extract_commands(response)
            if commands:
                _display_response_text(response, exclude_fences=True)
                if len(commands) > 1:
                    preview_and_execute_batch(commands, ctx)
                else:
                    preview_and_execute(commands[0], ctx)
            else:
                _display_response_text(response)

            print()

    except KeyboardInterrupt:
        pass

    print(f"\n{DIM}[termai] Goodbye.{RESET}")


def _handle_slash_command(
    text: str,
    ctx: "SessionContext",
    messages: list[dict[str, str]],
) -> None:
    cmd = text.lower().split()[0]

    if cmd == "/help":
        print(HELP_TEXT)

    elif cmd == "/history":
        if not ctx.history:
            print(f"  {DIM}(no commands executed this session){RESET}")
        else:
            print(f"\n  {BOLD}Session history:{RESET}")
            for i, entry in enumerate(ctx.history, 1):
                print(f"  {DIM}{i}.{RESET} {entry}")
        print()

    elif cmd == "/context":
        print(f"\n{DIM}{ctx.summary()}{RESET}\n")

    elif cmd == "/clear":
        messages.clear()
        print(f"  {DIM}Conversation cleared.{RESET}\n")

    else:
        registry = get_registry()
        plugin_commands = registry.get_slash_commands()
        if cmd in plugin_commands:
            args_str = text[len(cmd):].strip()
            plugin_commands[cmd](args_str, ctx)
        else:
            print(f"  Unknown command: {text}. Type /help for options.\n")


def _get_response(
    user_input: str,
    ctx: "SessionContext",
    model: LocalModel,
    messages: list[dict[str, str]],
) -> str:
    """Generate a response using the AI model or a simple fallback.

    Tries the local model first, then delegates to a remote provider
    if configured and the query appears complex.
    """
    system = CHAT_SYSTEM_PROMPT + "\n--- System Context ---\n" + ctx.summary() + "\n--- End Context ---"
    all_messages = messages + [{"role": "user", "content": user_input}]

    from termai.generator import _force_mode
    from termai.remote import get_remote_provider
    from termai.classifier import classify

    if _force_mode == "remote":
        remote = get_remote_provider()
        if remote and remote.is_available():
            return _try_remote_chat(remote, system, all_messages, user_input)

    local_result = None
    if model.is_available:
        raw = model.chat_generate(system, all_messages, max_tokens=512)
        if "--- System Context ---" in raw:
            raw = raw[:raw.index("--- System Context ---")].strip()
        local_result = raw

    if _force_mode == "local":
        return local_result or _chat_fallback(user_input)

    remote = get_remote_provider()
    if remote and remote.is_available():
        decision = classify(user_input, local_result, from_fallback=not model.is_available)
        if decision == "remote":
            remote_result = _try_remote_chat(remote, system, all_messages, user_input)
            if remote_result:
                return remote_result

    return local_result or _chat_fallback(user_input)


def _try_remote_chat(remote, system: str, messages: list[dict[str, str]], user_input: str) -> str | None:
    """Attempt to get a response from the remote AI provider."""
    print(f"  {CYAN}[remote]{RESET} {DIM}Processing...{RESET}")
    try:
        raw = remote.chat_generate(system, messages, max_tokens=512)
        if raw:
            print(f"  {CYAN}[remote]{RESET} {DIM}Response from remote AI{RESET}")
            return raw
    except Exception as e:
        print(f"  {YELLOW}[remote]{RESET} {DIM}Remote AI failed: {e}{RESET}")
    return None


def _chat_fallback(user_input: str) -> str:
    """Minimal keyword-based fallback for when no model is loaded."""
    lower = user_input.lower()

    if any(kw in lower for kw in ("list", "show", "files", "directory", "ls")):
        return "```bash\nls -la\n```"
    if any(kw in lower for kw in ("disk", "space", "storage")):
        return "```bash\ndf -h\n```"
    if any(kw in lower for kw in ("process", "running", "ps")):
        return "```bash\nps aux\n```"
    if any(kw in lower for kw in ("git", "status")):
        return "```bash\ngit status\n```"
    if any(kw in lower for kw in ("network", "ip", "interface")):
        return "```bash\nifconfig\n```"
    if any(kw in lower for kw in ("who", "am i", "username", "user")):
        return "```bash\nwhoami\n```"

    return (
        "I'm running without an AI model, so my capabilities are limited. "
        "Install gpt4all and a model for full conversational support.\n"
        "For now, try asking about files, disk space, processes, or git status."
    )


_FENCE_RE = re.compile(r"```(?:bash|sh|zsh)?\s*\n(.*?)\n```", re.DOTALL)


def _extract_commands(response: str) -> list[str]:
    """Pull shell commands out of ```bash fences in the AI response."""
    matches = _FENCE_RE.findall(response)
    commands: list[str] = []
    for block in matches:
        for line in block.strip().splitlines():
            line = line.strip()
            if line and not line.startswith("#"):
                commands.append(line)
    return commands


def _display_response_text(response: str, *, exclude_fences: bool = False) -> None:
    """Print the AI response, optionally stripping code fences."""
    text = response
    if exclude_fences:
        text = _FENCE_RE.sub("", text).strip()

    if text:
        print(f"\n{CYAN}{text}{RESET}")
